Objectif : Remplacer toute dépendance BD payante par un backend de stockage SQLite (fichier local persistant), avec TTL + export/import. Garde l’API existante.

À faire :

Backend de stockage

Ajoute core/storage.py avec un driver SQLiteStore (fichier data/vbs.db).

ENV : STORAGE_BACKEND=sqlite (par défaut), TTL_DRAFTS_DAYS=30, TTL_PUBLISH_LOG_DAYS=90.

Tables (si non existantes) :

drafts(id TEXT PK, user_id TEXT, title TEXT NOT NULL, description TEXT, price REAL NOT NULL CHECK(price>=0), brand TEXT, size TEXT, color TEXT, category TEXT, item_json TEXT, listing_json TEXT, flags_json TEXT, status TEXT CHECK(status IN ('pending','ready','prepared','published','error','manual')), created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP, updated_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP)

listings(id TEXT PK, user_id TEXT, vinted_id TEXT UNIQUE, title TEXT NOT NULL, price REAL NOT NULL, listing_url TEXT, status TEXT DEFAULT 'active', created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP, updated_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP)

publish_log(id TEXT PK, user_id TEXT, draft_id TEXT, idempotency_key TEXT UNIQUE, confirm_token TEXT NOT NULL, dry_run INTEGER NOT NULL DEFAULT 1, status TEXT DEFAULT 'queued', listing_url TEXT, error_json TEXT, created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP)

Index : idx_drafts_user, idx_drafts_status, idx_listings_user, idx_publog_idem.

Fonctions : save_draft(..), update_draft_status(..), get_drafts(..), log_publish(..), seen_idempotency(idem)->bool, vacuum_and_prune().

Intégration pipeline

Dans /bulk/generate : après quality gate, enregistrer le draft via save_draft; sinon retourner errors[] (aucun draft créé).

Dans /vinted/listings/publish :

Refuser si seen_idempotency(idem) (409).

log_publish(..., status='queued'), exécuter publish (dry-run par défaut), puis log_publish(..., status='ok|manual|error') et upsert listings + update_draft_status('published') si ok.

Jobs gratuits (APScheduler)

Tous les jours 02:00 : vacuum_and_prune() → supprime drafts published|error > TTL_DRAFTS_DAYS, purger publish_log > TTL_PUBLISH_LOG_DAYS, VACUUM.

Export/Import (gratuit)

GET /export/drafts?status=ready|all → renvoie un ZIP contenant drafts.json (liste minimale) + readme.txt.

POST /import/drafts (multipart zip ou json) → recrée les drafts (sans changer le plan).

Sécurité logs : garder le masquage cookie/UA (déjà fait). Aucune donnée sensible dans SQLite.

Doc : dans replit.md, note : “SQLite = gratuit & persistant sur la VM Replit. Pas de dépendance BD externe. TTL purge automatique.”