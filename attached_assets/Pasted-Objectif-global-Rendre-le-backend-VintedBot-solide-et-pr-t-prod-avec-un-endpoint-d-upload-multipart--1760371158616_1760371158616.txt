Objectif global
Rendre le backend VintedBot solide et prêt-prod avec un endpoint d’upload multipart (POST /api/v1/ingest/upload) compatible mobile, un pipeline image (orientation, compression, suppression EXIF GPS), un stockage local (Replit) ou S3-compatible, et la création automatique de brouillons (draft listing).
Respecter l’architecture propre, la sécurité (rate limit, taille max, MIME sniff), l’observabilité et les tests.

0) Dépendances & structure

Ajouter dans requirements.txt (versions récentes et stables) :

pillow
python-multipart
filetype
pydantic-settings
boto3
tenacity


Arborescence attendue (crée/complète si besoin) :

backend/
  app.py
  settings.py
  api/v1/routers/
    health.py
    ingest.py
    listings.py
  api/v1/schemas.py
  api/v1/errors.py
  core/
    db.py
    media.py
    security.py
    logging.py
    metrics.py
    rate_limit.py
    redis.py
  models/
    user.py
    media.py
    draft.py
    draft_photo.py
  migrations/
  tests/
    test_ingest_upload.py


API prefix : tout sous /api/v1.

1) Configuration (pydantic-settings)

backend/settings.py (ajouter ces champs) :

from pydantic_settings import BaseSettings
from typing import Literal, List

class Settings(BaseSettings):
    ENV: str = "dev"
    PORT: int = 8000
    DATABASE_URL: str = "sqlite+aiosqlite:///./data/app.db"
    REDIS_URL: str | None = None
    SECRET_KEY: str = "dev-secret"
    CORS_ORIGINS: List[str] = ["*"]
    # Media
    MEDIA_STORAGE: Literal["local","s3"] = "local"
    MEDIA_ROOT: str = "data/uploads"        # utilisé en local
    MEDIA_BASE_URL: str = "/media"          # base URL en local
    S3_BUCKET: str | None = None
    S3_ENDPOINT_URL: str | None = None      # pour MinIO/Wasabi
    S3_REGION: str | None = None
    S3_ACCESS_KEY: str | None = None
    S3_SECRET_KEY: str | None = None
    # Upload policy
    MAX_UPLOADS_PER_REQUEST: int = 20
    MAX_FILE_SIZE_MB: int = 15
    ALLOWED_MIME_PREFIXES: List[str] = ["image/"]
    JPEG_QUALITY: int = 80                 # 1..95
    MAX_DIM_PX: int = 1600
    STRIP_GPS: bool = True
    JOBS_ENABLED: bool = True

settings = Settings()


Créer le dossier data/uploads si MEDIA_STORAGE=local.

2) Modèles SQLAlchemy (draft + media)

backend/models/media.py :

from sqlalchemy.orm import Mapped, mapped_column
from sqlalchemy import String, Integer, DateTime, ForeignKey
from datetime import datetime
from .base import Base  # ou votre Base existante

class Media(Base):
    __tablename__ = "media"
    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
    user_id: Mapped[int | None] = mapped_column(ForeignKey("user.id"), nullable=True)
    sha256: Mapped[str] = mapped_column(String(64), index=True)
    filename: Mapped[str] = mapped_column(String(255))
    mime: Mapped[str] = mapped_column(String(64))
    width: Mapped[int] = mapped_column(Integer)
    height: Mapped[int] = mapped_column(Integer)
    size_bytes: Mapped[int] = mapped_column(Integer)
    storage: Mapped[str] = mapped_column(String(16))     # 'local' | 's3'
    url: Mapped[str] = mapped_column(String(512))
    created_at: Mapped[datetime] = mapped_column(default=datetime.utcnow)


backend/models/draft.py :

from sqlalchemy.orm import Mapped, mapped_column, relationship
from sqlalchemy import String, Integer, DateTime, ForeignKey, Float
from datetime import datetime
from .base import Base

class Draft(Base):
    __tablename__ = "draft"
    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
    user_id: Mapped[int | None] = mapped_column(ForeignKey("user.id"), nullable=True)
    title: Mapped[str] = mapped_column(String(160), default="")
    description: Mapped[str] = mapped_column(String(4000), default="")
    price_suggested: Mapped[float | None] = mapped_column(Float, nullable=True)
    status: Mapped[str] = mapped_column(String(24), default="draft")
    created_at: Mapped[datetime] = mapped_column(default=datetime.utcnow)
    photos: Mapped[list["DraftPhoto"]] = relationship(back_populates="draft", cascade="all, delete-orphan")


backend/models/draft_photo.py :

from sqlalchemy.orm import Mapped, mapped_column, relationship
from sqlalchemy import ForeignKey, Integer
from .base import Base

class DraftPhoto(Base):
    __tablename__ = "draft_photo"
    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
    draft_id: Mapped[int] = mapped_column(ForeignKey("draft.id"))
    media_id: Mapped[int] = mapped_column(ForeignKey("media.id"))
    order_index: Mapped[int] = mapped_column(Integer, default=0)
    draft: Mapped["Draft"] = relationship(back_populates="photos")


Créer migration Alembic pour ces tables.

3) Schémas Pydantic (sorties API)

backend/api/v1/schemas.py :

from pydantic import BaseModel
from typing import List, Optional

class MediaOut(BaseModel):
    id: int
    url: str
    width: int
    height: int
    mime: str

class DraftPhotoOut(BaseModel):
    media: MediaOut
    order_index: int

class DraftOut(BaseModel):
    id: int
    title: str
    description: str
    status: str
    price_suggested: Optional[float] = None
    photos: List[DraftPhotoOut]

4) Pipeline média (validation, EXIF, resize, stockage)

backend/core/media.py :

import os, io, hashlib
from typing import Tuple
import filetype
from PIL import Image, ImageOps
from backend.settings import settings

def _ensure_dirs():
    if settings.MEDIA_STORAGE == "local":
        os.makedirs(settings.MEDIA_ROOT, exist_ok=True)

def sniff_mime(data: bytes) -> str:
    kind = filetype.guess(data)
    return kind.mime if kind else "application/octet-stream"

def check_size_limit(data: bytes):
    mb = len(data) / (1024 * 1024)
    if mb > settings.MAX_FILE_SIZE_MB:
        raise ValueError(f"File too large: {mb:.1f} MB > {settings.MAX_FILE_SIZE_MB} MB")

def is_allowed_mime(mime: str) -> bool:
    return any(mime.startswith(p) for p in settings.ALLOWED_MIME_PREFIXES)

def process_image(data: bytes) -> Tuple[bytes, int, int, str]:
    """Corrige orientation EXIF, réduit à MAX_DIM_PX, encode en JPEG qualité réglée, retire EXIF."""
    img = Image.open(io.BytesIO(data))
    img = ImageOps.exif_transpose(img)  # orientation correcte
    img = img.convert("RGB")
    # resize
    w, h = img.size
    max_dim = settings.MAX_DIM_PX
    if max(w, h) > max_dim:
        if w >= h:
            nh = int(h * (max_dim / w))
            img = img.resize((max_dim, nh), Image.LANCZOS)
            w, h = max_dim, nh
        else:
            nw = int(w * (max_dim / h))
            img = img.resize((nw, max_dim), Image.LANCZOS)
            w, h = nw, max_dim
    # export sans EXIF
    out = io.BytesIO()
    img.save(out, format="JPEG", quality=settings.JPEG_QUALITY, optimize=True)
    return out.getvalue(), w, h, "image/jpeg"

def sha256_of(data: bytes) -> str:
    return hashlib.sha256(data).hexdigest()

def store_local(data: bytes, sha: str) -> str:
    _ensure_dirs()
    fname = f"{sha}.jpg"
    path = os.path.join(settings.MEDIA_ROOT, fname)
    if not os.path.exists(path):
        with open(path, "wb") as f:
            f.write(data)
    return f"{settings.MEDIA_BASE_URL}/{fname}"


Option S3 : si MEDIA_STORAGE="s3", ajouter une fonction store_s3(data, sha) utilisant boto3 (ACL privée, URL signée éventuelle). Pour ce sprint, implémente local, et laisse un TODO pour S3.

5) Endpoint d’upload (multipart)

backend/api/v1/routers/ingest.py :

from fastapi import APIRouter, UploadFile, File, Form, HTTPException, Depends
from typing import List
from backend.settings import settings
from backend.core.media import sniff_mime, check_size_limit, is_allowed_mime, process_image, sha256_of, store_local
from backend.core.db import async_session
from sqlalchemy import select
from backend.models.media import Media
from backend.models.draft import Draft
from backend.models.draft_photo import DraftPhoto
from backend.api.v1.schemas import DraftOut, DraftPhotoOut, MediaOut

router = APIRouter(tags=["ingest"])

@router.post("/ingest/upload", response_model=DraftOut, status_code=201)
async def ingest_upload(
    files: List[UploadFile] = File(..., description="field name 'files' (multiple)"),
    title: str = Form("", description="optional initial title"),
):
    if not files:
        raise HTTPException(400, "No files provided")
    if len(files) > settings.MAX_UPLOADS_PER_REQUEST:
        raise HTTPException(413, f"Too many files (>{settings.MAX_UPLOADS_PER_REQUEST})")

    # Lire et traiter chaque image
    processed = []
    for f in files:
        raw = await f.read()
        check_size_limit(raw)
        mime = sniff_mime(raw)
        if not is_allowed_mime(mime):
            raise HTTPException(415, f"Unsupported MIME: {mime}")
        jpeg_bytes, w, h, out_mime = process_image(raw)
        sha = sha256_of(jpeg_bytes)  # idempotence par contenu final
        url = store_local(jpeg_bytes, sha)
        processed.append((sha, url, w, h, out_mime, len(jpeg_bytes)))

    # Enregistrer en DB
    async with async_session() as session:
        draft = Draft(title=title or "", description="", status="draft")
        session.add(draft)
        await session.flush()

        order = 0
        for (sha, url, w, h, mime, size_bytes) in processed:
            media = Media(sha256=sha, filename=f"{sha}.jpg", mime=mime, width=w, height=h,
                          size_bytes=size_bytes, storage="local", url=url)
            session.add(media)
            await session.flush()
            dp = DraftPhoto(draft_id=draft.id, media_id=media.id, order_index=order)
            session.add(dp)
            order += 1

        await session.commit()

    # DTO
    photos_out = [
        DraftPhotoOut(
            media=MediaOut(id=i+1, url=p[1], width=p[2], height=p[3], mime=p[4]),
            order_index=idx,
        )
        for idx, p in enumerate(processed)
    ]
    return DraftOut(id=draft.id, title=draft.title, description=draft.description,
                    status=draft.status, price_suggested=None, photos=photos_out)


Notes :

Champ multipart files (multiple) — exactement ce que le front enverra.

Limites et contrôles : nombre de fichiers, taille max, sniff MIME, conversion JPEG, orientation, EXIF retiré.

Idempotence : hash sur le contenu traité (sha256), réutilisable plus tard pour éviter les doublons.

Stockage local par défaut (/media/<sha>.jpg).

6) Exposer les médias locaux

Dans app.py : monter le répertoire statique pour /media.

from fastapi.staticfiles import StaticFiles
from backend.settings import settings

if settings.MEDIA_STORAGE == "local":
    app.mount(settings.MEDIA_BASE_URL, StaticFiles(directory=settings.MEDIA_ROOT), name="media")


Et inclure le router :

from backend.api.v1.routers import ingest
app.include_router(ingest.router, prefix="/api/v1")

7) Sécurité & robustesse

Rate limit global et par endpoint (slowapi) : ex. 60/min global, 10/min sur /ingest/upload.

Taille requête : vérifier la taille en lisant UploadFile (déjà fait via check_size_limit), et retourner 413 si dépassement.

CORS : whitelist configurable via settings.CORS_ORIGINS.

Timeouts I/O externes (si on ajoute téléchargement d’URL dans un autre endpoint).

Logs : logguer sha, mime, taille, user_id (si auth).

8) Health & intégration front

GET /api/v1/health répond {"status":"ok"} (déjà en place côté health).

Le front appelle POST /api/v1/ingest/upload → reçoit un DraftOut (id + photos).

Garder /ingest/photos (URLs) existant si présent, sinon TODO.

9) Tests Pytest

backend/tests/test_ingest_upload.py (créer un petit JPEG en mémoire) :

import io
from PIL import Image
from httpx import AsyncClient
from backend.app import app

def make_image_bytes(w=800,h=600):
    img = Image.new("RGB", (w,h), (200,200,200))
    buf = io.BytesIO()
    img.save(buf, format="JPEG", quality=80)
    return buf.getvalue()

import pytest
@pytest.mark.asyncio
async def test_ingest_upload_ok():
    data = make_image_bytes()
    files = {"files": ("test.jpg", data, "image/jpeg")}
    async with AsyncClient(app=app, base_url="http://test") as ac:
        r = await ac.post("/api/v1/ingest/upload", files=files)
    assert r.status_code == 201, r.text
    js = r.json()
    assert js["status"] == "draft"
    assert len(js["photos"]) == 1
    assert js["photos"][0]["media"]["url"].startswith("/media/")

10) Critères d’acceptation (DoD)

POST /api/v1/ingest/upload :

Accepte 1..20 images, retourne 201 + DraftOut avec la liste des photos.

Chaque photo est orientée correctement, redimensionnée à MAX_DIM_PX, encodée en JPEG qualité JPEG_QUALITY et EXIF GPS supprimé.

Rejet clair (415) si MIME non image, (413) si taille > MAX_FILE_SIZE_MB.

Stockage local sous data/uploads + servi sous GET /media/<sha>.jpg.

Health /api/v1/health = OK ; CORS actif ; rate limit appliqué.

Tests passent : pytest -q backend/tests/test_ingest_upload.py.

Applique ces modifications, liste les fichiers modifiés/créés, lance les tests et démarre l’app.
Ensuite, affiche, à la fin des logs :
✅ /ingest/upload ready | storage=local | MAX_DIM=1600 | JPEG_QUALITY=80