"""
Optimized AI Service with Cost Management and Fallback
Reduces OpenAI costs by 90% using intelligent model selection
"""
import os
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from openai import AsyncOpenAI, OpenAI
from backend.utils.logger import logger
from backend.core.redis_client import cache

# OpenAI Configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY, timeout=60.0, max_retries=2)

# Model Configuration - Automatic fallback from expensive to cheap
MODELS = {
    "premium": {
        "name": "gpt-4o",  # Latest GPT-4 with vision
        "cost_per_1k_input": 0.0025,  # $2.50 per 1M input tokens
        "cost_per_1k_output": 0.01,    # $10 per 1M output tokens
        "max_tokens": 16384,
        "vision": True,
    },
    "standard": {
        "name": "gpt-4o-mini",  # 90% cheaper, still excellent
        "cost_per_1k_input": 0.00015,  # $0.15 per 1M input tokens
        "cost_per_1k_output": 0.0006,   # $0.60 per 1M output tokens
        "max_tokens": 16384,
        "vision": True,
    },
    "budget": {
        "name": "gpt-3.5-turbo",  # Cheapest, text-only
        "cost_per_1k_input": 0.0005,
        "cost_per_1k_output": 0.0015,
        "max_tokens": 4096,
        "vision": False,
    }
}

# Cost Limits
COST_LIMIT_PER_USER_DAILY = float(os.getenv("OPENAI_COST_LIMIT_PER_USER", "5.0"))  # $5 per day per user
COST_LIMIT_GLOBAL_DAILY = float(os.getenv("OPENAI_COST_LIMIT_GLOBAL", "500.0"))    # $500 per day total


class AIOptimizer:
    """
    Intelligent AI service with automatic cost optimization

    Features:
    - Automatic model selection based on budget
    - Per-user daily spending limits
    - Global daily spending cap
    - Cost tracking and alerts
    - Automatic fallback to cheaper models
    - Response caching to reduce API calls
    """

    def __init__(self):
        self.client = openai_client

    async def _get_user_spending(self, user_id: str) -> float:
        """Get user's spending today"""
        cache_key = f"ai_spending:user:{user_id}:{datetime.utcnow().date()}"
        spending = await cache.get(cache_key, default=0.0)
        return float(spending)

    async def _get_global_spending(self) -> float:
        """Get global spending today"""
        cache_key = f"ai_spending:global:{datetime.utcnow().date()}"
        spending = await cache.get(cache_key, default=0.0)
        return float(spending)

    async def _track_spending(self, user_id: str, cost: float):
        """Track user and global spending"""
        today = datetime.utcnow().date()

        # Track user spending
        user_key = f"ai_spending:user:{user_id}:{today}"
        await cache.increment(user_key, int(cost * 10000))  # Store as cents (0.0001 precision)
        await cache.expire(user_key, timedelta(days=2))  # Expire after 2 days

        # Track global spending
        global_key = f"ai_spending:global:{today}"
        await cache.increment(global_key, int(cost * 10000))
        await cache.expire(global_key, timedelta(days=2))

        # Alert if limits exceeded
        user_spending = await self._get_user_spending(user_id)
        global_spending = await self._get_global_spending()

        if user_spending > COST_LIMIT_PER_USER_DAILY:
            logger.warning(f"[WARN] User {user_id} exceeded daily limit: ${user_spending:.2f}")

        if global_spending > COST_LIMIT_GLOBAL_DAILY:
            logger.error(f"ðŸš¨ GLOBAL SPENDING LIMIT EXCEEDED: ${global_spending:.2f}")

    def _calculate_cost(self, model_config: dict, input_tokens: int, output_tokens: int) -> float:
        """Calculate cost for API call"""
        input_cost = (input_tokens / 1000) * model_config["cost_per_1k_input"]
        output_cost = (output_tokens / 1000) * model_config["cost_per_1k_output"]
        return input_cost + output_cost

    async def _select_model(self, user_id: str, requires_vision: bool = False) -> dict:
        """
        Select best model based on budget and requirements

        Returns model config or raises exception if user over budget
        """
        user_spending = await self._get_user_spending(user_id)
        global_spending = await self._get_global_spending()

        # Check hard limits
        if user_spending >= COST_LIMIT_PER_USER_DAILY:
            raise Exception(
                f"Daily AI budget exceeded (${user_spending:.2f}/${COST_LIMIT_PER_USER_DAILY:.2f}). "
                f"Resets at midnight UTC."
            )

        if global_spending >= COST_LIMIT_GLOBAL_DAILY:
            raise Exception(
                "Service temporarily unavailable due to high demand. Please try again later."
            )

        # Select model based on remaining budget
        remaining_budget = COST_LIMIT_PER_USER_DAILY - user_spending

        # Premium model if budget allows and user hasn't used much
        if remaining_budget > 2.0 and user_spending < 1.0 and requires_vision:
            logger.info(f"Using premium model (gpt-4o) for user {user_id}")
            return MODELS["premium"]

        # Standard model (default for most users - 90% cheaper than premium)
        if requires_vision:
            logger.info(f"Using standard model (gpt-4o-mini) for user {user_id}")
            return MODELS["standard"]

        # Budget model if no vision needed
        logger.info(f"Using budget model (gpt-3.5-turbo) for user {user_id}")
        return MODELS["budget"]

    async def analyze_photos(
        self,
        user_id: str,
        photo_paths: List[str],
        custom_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Analyze clothing photos with automatic cost optimization

        Args:
            user_id: User identifier for cost tracking
            photo_paths: List of image paths
            custom_prompt: Optional custom system prompt

        Returns:
            Analysis result with pricing info
        """
        # Check cache first
        cache_key = f"ai_analysis:{user_id}:{':'.join(sorted(photo_paths))}"
        cached = await cache.get(cache_key)
        if cached:
            logger.info(f"[OK] Cache hit for user {user_id} - $0.00 cost")
            return cached

        # Select model
        model_config = await self._select_model(user_id, requires_vision=True)

        try:
            # Prepare images
            import base64
            from pathlib import Path

            image_contents = []
            for path in photo_paths[:6]:  # Limit to 6 photos
                if not Path(path).exists():
                    continue

                # Convert HEIC if needed
                if path.lower().endswith(('.heic', '.heif')):
                    from backend.core.ai_analyzer import convert_heic_to_jpeg
                    path = convert_heic_to_jpeg(path)

                with open(path, "rb") as f:
                    base64_image = base64.b64encode(f.read()).decode('utf-8')
                    image_contents.append({
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}",
                            "detail": "high"
                        }
                    })

            # System prompt (optimized for conciseness to reduce tokens)
            system_prompt = custom_prompt or """You are a clothing analysis AI. Analyze photos and return JSON:
{
  "title": "short catchy title (60-90 chars)",
  "description": "5-7 bullet points with emoji",
  "price": suggested_price_euros,
  "category": "hoodie|t-shirt|jeans|etc",
  "brand": "brand_name or unknown",
  "size": "S|M|L|XL etc",
  "color": "dominant_color",
  "condition": "new|very_good|good|satisfactory",
  "materials": ["cotton", "polyester"],
  "fit": "oversized|regular|slim",
  "confidence": 0-1
}"""

            # API call
            response = await self.client.chat.completions.create(
                model=model_config["name"],
                messages=[
                    {"role": "system", "content": system_prompt},
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": "Analyze these clothing photos:"},
                            *image_contents
                        ]
                    }
                ],
                max_tokens=1000,  # Limit output tokens to control cost
                temperature=0.7,
                response_format={"type": "json_object"}
            )

            # Calculate cost
            usage = response.usage
            cost = self._calculate_cost(
                model_config,
                usage.prompt_tokens,
                usage.completion_tokens
            )

            # Track spending
            await self._track_spending(user_id, cost)

            # Parse result
            import json
            result = json.loads(response.choices[0].message.content)

            # Add metadata
            result["_metadata"] = {
                "model": model_config["name"],
                "cost": round(cost, 4),
                "tokens": {
                    "input": usage.prompt_tokens,
                    "output": usage.completion_tokens,
                    "total": usage.total_tokens
                }
            }

            # Cache for 24 hours
            await cache.set(cache_key, result, ttl=timedelta(hours=24))

            logger.info(
                f"[OK] Analysis complete: model={model_config['name']}, "
                f"cost=${cost:.4f}, tokens={usage.total_tokens}"
            )

            return result

        except Exception as e:
            logger.error(f"[ERROR] AI analysis failed: {e}")

            # Fallback to cheaper model if premium failed
            if model_config["name"] == MODELS["premium"]["name"]:
                logger.warning("[WARN] Retrying with standard model")
                model_config = MODELS["standard"]
                # Recursive call with standard model
                return await self.analyze_photos(user_id, photo_paths, custom_prompt)

            raise

    async def chat_completion(
        self,
        user_id: str,
        messages: List[Dict[str, str]],
        max_tokens: int = 500
    ) -> str:
        """
        General chat completion with cost tracking

        Args:
            user_id: User identifier
            messages: Chat messages
            max_tokens: Max output tokens

        Returns:
            AI response text
        """
        # Select model (no vision needed)
        model_config = await self._select_model(user_id, requires_vision=False)

        response = await self.client.chat.completions.create(
            model=model_config["name"],
            messages=messages,
            max_tokens=max_tokens,
            temperature=0.7
        )

        # Calculate and track cost
        usage = response.usage
        cost = self._calculate_cost(model_config, usage.prompt_tokens, usage.completion_tokens)
        await self._track_spending(user_id, cost)

        logger.info(f"Chat completion: model={model_config['name']}, cost=${cost:.4f}")

        return response.choices[0].message.content

    async def get_user_stats(self, user_id: str) -> dict:
        """Get user's AI usage statistics"""
        today_spending = await self._get_user_spending(user_id)

        return {
            "spending_today": round(today_spending, 2),
            "daily_limit": COST_LIMIT_PER_USER_DAILY,
            "remaining_budget": round(COST_LIMIT_PER_USER_DAILY - today_spending, 2),
            "percentage_used": round((today_spending / COST_LIMIT_PER_USER_DAILY) * 100, 1)
        }

    async def get_global_stats(self) -> dict:
        """Get global AI usage statistics"""
        today_spending = await self._get_global_spending()

        return {
            "spending_today": round(today_spending, 2),
            "daily_limit": COST_LIMIT_GLOBAL_DAILY,
            "percentage_used": round((today_spending / COST_LIMIT_GLOBAL_DAILY) * 100, 1)
        }


# Global AI optimizer instance
ai = AIOptimizer()
